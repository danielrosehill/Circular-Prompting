# Voice Note: Circular Prompting Technique

> The user describes a prompting technique called "Circular Prompting" for use with AI agents, particularly in code remediation scenarios. This method involves repeated prompts to refine a codebase iteratively, and using the end point in the context window to then restart the prompt.

*Transcribed: 31 Dec 2025 12:52*

---

All right, so the objective of this, um, audio context data, i.e., uh, voice recording, is to gather some context data for, um, a, well, I guess it's a method that I've been kind of trying out lately with different AI agents and which I've always kind of thought that might actually work really well. Um, I'm not sure if I've seen this described with a name. Um, there's so many different, uh, prompt engineering techniques that I'd imagine this does have a name, in which case, uh, I can use this context data to find the name and then update my notes with, uh, using the the established, uh, nomenclature. And if not, uh, then I guess I can think of something. But the one that kind of came to my mind, which is why I've called it this, is circular prompting. I'll describe what I was intending by that. So I'm currently in the process of, um, working on this home inventory system, one of those AI agent and vibe coding things that's just become, uh, way more time-consuming than I had hoped and which I had time for. Um, but I'm just trying to finish off the finish off the job, and it's been interesting to, uh, use, try out, uh, use it as a kind of a jumping-off point to try out. I'm looking at my whiteboard here. Uh, Kimmy, Gwen, Mini Max, Jomi's model, um, all the interesting stuff coming out of the east at the moment, uh, to see how how all of it can do. Um, I have to say horribly failed experiment because I think, uh, unfortunately, it was precisely at the point that it was finally in a stable, uh, the code base was finally in a stable situation that I said, oh, I wonder that, well, it's working now. Let's let's try, let's try one of these things to, you know, to to add these few more bells and whistles, and that was when I think, uh, all disaster struck. Uh, but Claude code is remediating this, and I think it's going beyond, I think we've now gone beyond very quickly, um, remediating errors to doing a process that to me is what I'd like to see AI agents do on a code base and I think is when they're at their best. And that is you throw at it a pretty general prompt. Could be injecting a bit of context. So the context that I'm, I think I even have on my clipboard now because I've been saying it so many times is, this is a project that was, um, Goose ORM and we moved over to Drizzle ORM. So, the starting point here was a back-end refactor. Um, it was very the type definitions in the original project were extremely strict, and I thought, this is a relational database system. It's for inventory. There has to be an easier way than this. So, uh, Claus and I agreed upon a refactor to Drizzle and, um, basically, a bunch of little back-end bugs. Now, to the credit of the method and the success of Claude Opus 4.5, the refactor was 90 5% successful, which to me probably approximates real-world development. I mean, if you asked a developer or back-end developers to rewrite an entire code base to, um, from Go to Drizzle, unless they were on, uh, you know, Red Bull times 100. I don't think anyone could get that job done in 30 minutes, which is what Claude did. And if they did, I'd do it under that extreme ridiculous time confine. There probably would be a whole bunch of little bugs like that. So I think it was actually I don't want to judge it too harshly. I think that was a that was actually a fairly successful refactor. But, of course, there is little problems with, you know, so many different API endpoints in the back-end. So, basically, the prompting that I've been doing uses that little bit of context. Um, this is the what we're working with. This is what we just did. So, basically saying that these errors that we're seeing are not just kind of, um, yes, they are individual errors, but they're manifestation of a broader pattern. And likewise, your task in remediating isn't just to put out these small fires, these tiny bugs. It's to clean up the code base and make sure we're post migration to Drizzle in a very, very robust and strong and solid position. Now, to come back to the idea for this repository, the circular method I'm describing, context is going to be running down. That's always the challenge at the moment, is figuring out ways to clean the context window and prevent context, uh, pollution. So, in in Claude, their default method of doing that at the moment with compacting the conversation, theoretically, that's kind of a way to continue a thread indefinitely without having to repeat context. Um, I haven't seen it work that well. I feel like even when you do the compacting, um, I feel like there's still some kind of context degradation going on. I don't, and I'm and I'm just saying this from my personal usage anecdotal experience using it, but I still feel like the the most, I think I get better results when I just start a new threat. So, the the circular prompt in this case would involve or the technique and strategy would involve repeating this prompt, knowing when to call and what when to to find a new turn. That's the critical thing. So Claude or your average agent might say, great, I've done this. Do you want me to keep going through the code base? And you have to know when this is where the tooling would come in. When the context window is at 60%, you're like, great, definitely I want to take you up on that offer, but I'm going to start a new thread and we're going to start from the top. Um, so this this pattern can be used for stuff like this, things like this, or things that are more evergreen. To give an example, an evergreen prompts might be not related to a specific project. It's like, go through this code base, make sure, give it a, give it a quality review, you know, make sure that there's no, um, this is the stack. Make sure that there's no old code, legacy code in it. And the circular prompting method that I'm envisioning, I love how in Claude code you have the up arrow so you can see your last prompt. So basically, you run that, wait for Claude to make a bit of progress, stop that conversation, new conversation, run the prompt again. Bit more progress, stop, run. Bit more progress, stop, run until and what what I would hope, this would be the experimentary method that you reach the point that Claude says, oh, everything's actually there's there's no work left to do. The code base is clean, no legacy code detected, we're good. And that is the success criteria. So it's basically instead of instead of trying to keep, to compact the context, to keep one long thread and use that method to keep within the context window, it would be a method of prompting that, and again, at the moment, I'm thinking this has to be human manually initiated, but perhaps it could be agent operated by a planning agent saying, you're good. Same prompt again, stop. Go again, stop. Go again, stop. In a loop until we reach that point. That's really what I wanted to, uh, to document here and to describe. And hopefully, I'll be able to use the wonderful power of AI, uh, with transcription and content reformatting to bring a bit of order and clarity to those thoughts.
