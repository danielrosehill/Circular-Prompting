# Circular Prompting for AI Agents: Idea Documentation

> The user documents an idea called 'circular prompting' for AI agents. The idea involves repeating prompts to guide AI agents through tasks like code review, managing context windows by starting new threads rather than compacting existing ones.

*Transcribed: 31 Dec 2025 12:52*

---

# Objective

All right, so the objective of this audio context data, voice recording, is to gather some context data for a—well, I guess it's a method that I've been trying out lately with different AI agents, which I've always thought might actually work really well. I'm not sure if I've seen this described with a name.

There are so many different prompt engineering techniques that I imagine this does have a name. In which case, I can use this context data to find the name and then update my notes with using the established nomenclature. If not, then I guess I can think of something, but the one that came to my mind—that's why I called it this—is circular prompting. I'll describe what I was intending by that. So, I'm currently in the process of working on this home inventory system, one of those AI agent and vibe coding things that's just become way more time-consuming than I had hoped and which I had time for. I'm just trying to finish off the job, and it's been interesting to try out, use it as a jumping-off point to try out:

I'm looking at my whiteboard here. Kimi, Queen, Mini Max, Xiaomi's model, all the interesting stuff coming out of the East at the moment to see how all of it can do. I have to say, horribly failed experiment because I think unfortunately it was precisely at the point that it was finally in a stable, the code base was finally in a stable situation that I said, "Oh, I wonder—well, it's working now. Let's try one of these things to add these few more bells and whistles." That was when I think all disaster struck. Claude Code is remediating this, and I think it's going beyond. I think we've now gone beyond very quickly remediating errors to doing a process that to me is what I like to see AI agents do on a code base. I think it's when they're at their best. That is, you throw at it a pretty general prompt. It could be injecting a bit of context.

# Project Refactor Context

The context that I think I even have in my clipboard now because I've been saying it so many times is this is a project that was Goose ORM, and we moved over to Drizzle ORM. So the starting point here was a backend refactor. It was very, the type definitions in the original project were extremely strict, and I thought this is a relational database system. It's for inventory. There has to be an easier way than this. So Claude and I agreed upon a refactor to Drizzle and basically a bunch of little backend bugs.

Now to the credit of the method and the success of Claude Opus 4.5, the refactor was 95% successful, which to me probably approximates real-world development. I mean, if you asked a developer or backend developers to rewrite an entire code base from Go to Drizzle, unless they were on, you know, Red Bull times 100, I don't think anyone could get that job done in 30 minutes, which is what Claude did. If they did do it under that extreme ridiculous time confined, there probably would be a whole bunch of little bugs like that. So I think it was actually, I don't want to judge it too harshly. I think that was actually a fairly successful refactor. Of course, there are little problems with so many different API endpoints in the backend. So basically the prompting that I've been doing uses that a little bit of context.

This is what we're working with. This is what we just did. So basically saying that these errors that we're seeing are not just kind of—yes, they are individual errors, but they're a manifestation of a broader pattern. Likewise, your task and remediating isn't just to put out these small fires, these tiny bugs. It's to clean up the code base and make sure we're post-migration to Drizzle in a very, very robust and strong and solid position.

# Circular Prompting

Now, to come back to the idea for this repository, the circular method I'm describing, context is going to be running down. That's always the challenge at the moment is figuring out ways to clean the context window and prevent context pollution. So in Claude, their default method of doing that at the moment with compacting the conversation, theoretically, that's a way to continue a thread indefinitely without having to repeat context. I haven't seen it work that well. I feel like even when you do the compacting, I feel like there's still some kind of context degradation going on. I don't—I'm just saying this from my personal usage anecdotal experience of using it, but I still feel like the the most I think I get better results when I just start a new thread.

So, the repeat, the circular prompt in this case would involve or the technique and strategy would involve repeating this prompt, knowing when to call and when to define a new turn. That's the critical thing. Claude or your average agent might say, "Great, I've done this. Do you want me to keep going through the code base?" You have to know when—this is where the tooling would come in—when the context window is at 60%, you're like, "Great. I want to take you up on that offer, but I'm going to start a new thread, and we're going to start from the top." So this pattern can be used for stuff like this, things like this, or things that are more evergreen. To give an example, an evergreen prompt might be not related to a specific project, just like, go through this code base, make sure give it a give it a quality review. Make sure that there's no, this is the stack. Make sure that there's no old code, legacy code in it. The circular prompting method that I'm envisioning—I love how in Claude Code you have the up arrow, so you can see your last prompt.

So basically, you run that, wait for Claude to make a bit of progress, stop that conversation, new conversation, run the prompt again. A bit more progress, stop, run. A bit more progress, stop, run until—and what I would hope, this would be the experimental method—that you reach the point that Claude says, "Oh, everything's actually, there's no work left to do. The code base is clean. No legacy code detected. We're good." That is the success criteria. So it's basically instead of trying to keep to compact the context, to keep one long thread and use that method to keep within the context window, it would be a method of prompting that—and again, at the moment, I'm thinking this has to be human manually initiated, but perhaps it could be agent-operated by a planning agent, saying, "You're good. Same prompt again, stop. Go again, stop. Go again, stop." In a loop until we reach that point. That's really what I wanted to document here and to describe. Hopefully I'll be able to use the wonderful power of AI with transcription and content reformatting to bring a bit of order and clarity to those thoughts.

